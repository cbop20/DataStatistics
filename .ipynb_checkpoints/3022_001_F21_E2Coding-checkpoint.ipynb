{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# CSCI 3022: Intro to Data Science - Fall 2021 Final Exam Coding\n",
    "***\n",
    "\n",
    "This exam coding segment is due on Canvas by **11:59 PM on Tuesday December 14*. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  \n",
    "\n",
    "\n",
    "Note: There are a number of images in this problem.  If you can't get them to display properly, they are hosted [here](https://drive.google.com/drive/folders/1M4kBMd23roiDVnfbYQmPmzsR1bPZAxzX?usp=sharing).  \n",
    "\n",
    "**Here are the rules:** \n",
    "\n",
    "1. All work, code and analysis, must be your own. \n",
    "2. You may use your course notes, posted lecture slides, textbooks, in-class notebooks, and homework solutions as resources.  You may also search online for answers to general knowledge questions like the form of a probability distribution function or how to perform a particular operation in Python/Pandas. \n",
    "3. This is meant to be like a coding portion of your midterm exam. So, the instructional team will be a bit less helpful than we typically are with homework. For example, we will not check answers, significantly debug your code, and so on.  But please don't feel like you're totally alone on this: feel free to ask questions or ask for help, and we will decide how best to provide that assistance.\n",
    "4. If something is left open-ended, it is because we want to see how you approach the kinds of problems you will encounter in the wild, where it will not always be clear what sort of tests/methods should be applied. Feel free to ask clarifying questions though.\n",
    "5. You may **NOT** post to message boards or other online resources asking for help.\n",
    "6. You may **NOT** copy-paste solutions *from anywhere*.\n",
    "7. You may **NOT** collaborate with classmates or anyone else.\n",
    "8. In short, **your work must be your own**. It really is that simple.\n",
    "\n",
    "Violation of the above rules will result in an immediate academic sanction (*at the very least*, you will receive a 0 on this exam coding assignment or an F in the course, depending on severity), and a trip to the Honor Code Council.\n",
    "\n",
    "**By submitting this assignment, you agree to abide by the rules given above.**\n",
    "\n",
    "***\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- You may not use late days on the exam coding nor can you drop your exam coding grades. \n",
    "- If you have a question for us, post it as a **PRIVATE** message on Piazza.  If we decide that the question is appropriate for the entire class, then we will add it to a exam coding clarifications thread. (NB: you should use public posts for the epidemiology \"warmup\" prompts).\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. The exam coding assignmnet is designed to be completed using only the packages in the first given code cell.\n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Canvas.  Do not compress it using tar, rar, zip, etc. \n",
    "- This should go without saying, but... For any question that asks you to calculate something, you **must show all work to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit.\n",
    "\n",
    "---\n",
    "**Shortcuts:**  [Problem 1](#p1) | [Problem 2](#p2) | [Problem 3](#p2) | [Bottom](#bot)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import matplotlib.pylab as plt \n",
    "from patsy import dmatrices\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p1'></a>\n",
    "\n",
    "## [30 points] Problem 1: Automating a fit\n",
    "\n",
    "** 1 A: Here's some data!**\n",
    "\n",
    "Load the data in `not_so_linear.csv` and create an (x,y) scatter plot of it.  \n",
    "\n",
    "The data come from a  NIST study, where the variable `y` is ultrasonic response when subjected to `x` of metal distance.\n",
    "\n",
    "It's pretty clear that $y$ is **a** function of $x$, but it's not a straight line!  So we may want to ask: what is the **best** polynomial of the form $$y=\\beta_0+\\beta_1\\cdot x +\\beta_2\\cdot x ^2 +\\beta_3\\cdot x ^3 +\\dots +\\beta_n\\cdot x^n$$\n",
    "to capture the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.9</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78.7</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64.2</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64.9</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57.1</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y      x\n",
       "0  92.9  0.500\n",
       "1  78.7  0.625\n",
       "2  64.2  0.750\n",
       "3  64.9  0.875\n",
       "4  57.1  1.000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plot data here.\n",
    "df=pd.read_csv('./data/not_so_linear.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1 B: find the best polynomial fit **\n",
    "\n",
    "Our goal is to find how many powers of $x$ we need to capture the curve well.  Recall that if we add extra *predictors* or powers of $x$ to a model the SSE and $R^2$ of that model will always appear better.  But at some point, they don't improve by enough to be \"worth\" the extra terms in the model.\n",
    "\n",
    "Write a function `best_poly(df)` with the following usage:\n",
    "- Argument: the original data frame `df` with columns `y` and `x`\n",
    "- Your function should return a stats.OLS.fit() argument that corresponds to the **best** fit $y=X\\beta$ according adjusted $R^2$ where you consider including up to 10 powers of X.  \n",
    "\n",
    "In other words, your function should fit **all 11** of the linear models \n",
    "\n",
    "Model 0: $y=\\beta_0$, \n",
    "\n",
    "Model 1: $y=\\beta_0+\\beta_1x^1$, \n",
    "\n",
    "Model 2: $y=\\beta_0+\\beta_1x^1+\\beta_2x^2$, \n",
    "\n",
    "$\\vdots $\n",
    "\n",
    "Model 10: $y=\\beta_0+\\beta_1x^1+\\beta_2x^2+\\dots+\\beta_{10}x^{10}$\n",
    "\n",
    "\n",
    "and then `return` the one with the best adjusted R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_poly(dfi):\n",
    "    form = ''\n",
    "    model = sm.OLS(df['y'],sm.add_constant(df['x'])).fit()\n",
    "    adjrsq = model\n",
    "    \n",
    "    for i in range(1,10):\n",
    "        form = form +'+I(x**{})'.format(i)\n",
    "        y,x = dmatrices('y ~' + form,data=df,return_type='dataframe')\n",
    "        model = sm.OLS(y,x).fit()\n",
    "        if(model.rsquared_adj > adjrsq.rsquared_adj):\n",
    "            adjrsq = model\n",
    "    \n",
    "    return adjrsq\n",
    "winninglm=best_poly(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 C: Visualize your results**\n",
    "\n",
    "For the best model by adjusted R-squared, print the `summary()` table and make a plot with the overlay the resulting fitted line over the data.  Does it appear as though we appropriately captured the structure of the data?\n",
    "\n",
    "**Note:** Even if you can't get the function to \"automate\" the best polynomial in 1B, you should be able to do this part by hand if you need it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.981\n",
      "Model:                            OLS   Adj. R-squared:                  0.981\n",
      "Method:                 Least Squares   F-statistic:                     2163.\n",
      "Date:                Wed, 15 Dec 2021   Prob (F-statistic):          3.69e-177\n",
      "Time:                        03:24:16   Log-Likelihood:                -555.55\n",
      "No. Observations:                 214   AIC:                             1123.\n",
      "Df Residuals:                     208   BIC:                             1143.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    127.2106      3.574     35.593      0.000     120.165     134.256\n",
      "I(x ** 1)   -121.9559      9.626    -12.670      0.000    -140.933    -102.979\n",
      "I(x ** 2)     58.1454      8.508      6.834      0.000      41.372      74.919\n",
      "I(x ** 3)    -14.8854      3.261     -4.565      0.000     -21.313      -8.457\n",
      "I(x ** 4)      1.9237      0.559      3.443      0.001       0.822       3.025\n",
      "I(x ** 5)     -0.0978      0.035     -2.791      0.006      -0.167      -0.029\n",
      "==============================================================================\n",
      "Omnibus:                       42.672   Durbin-Watson:                   0.933\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              152.633\n",
      "Skew:                           0.746   Prob(JB):                     7.18e-34\n",
      "Kurtosis:                       6.859   Cond. No.                     1.44e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.44e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2619a537df0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(winninglm.summary())\n",
    "plt.scatter(winninglm.fittedvalues,df['x'])\n",
    "plt.scatter(df['y'],df['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p2'></a>\n",
    "\n",
    "## [30 points] Problem 2: Testing for Streaks\n",
    "\n",
    "One measure we're often interested in for modeling is whether or not our data is exhibiting patterns, where nearby observations behave similarly to one another.  We explored one case of this briefly in the last problem of HW9, and to date have used simple plots of residuals to determine whether not the exhibit clumping, where positive and negatively signed residuals tend to appear in grouping of the same type.  In practice, this is quite common whenever one of your $X$ *predictor* variables is either **location** or **time**, since nearby data points in either of those tend to have similar errors.\n",
    "\n",
    "In tihs problem we'll explore a simplified variant of a common diagnostic for whether or a not a list of number is exhibiting streakiness in the clumping of positive and negative values, the [Wald-Wolfowitz Runs Test](https://en.wikipedia.org/wiki/Wald%E2%80%93Wolfowitz_runs_test).  I recommend skimming the link before you start.\n",
    "\n",
    "Instead of the exact statistic of the Wald test, we're going to come up with two other measures to create a hypothesis test of whether or not a list of signed numbers are exhibiting regular amounts of clumping or not.  Before we can create any kind of hypothesis test, we should write a function to count the runs in a list of numbers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2 A: Implementing some statistics**\n",
    "\n",
    "    \n",
    "Implement a function `count_runs` that takes a list (with length at least 2) of `float` values.  It should return `count, longest`, where\n",
    "\n",
    "- `count`: The number of distinct + or - runs in that list\n",
    "- `longest`:The length of the single longest streak of same-signed values in the list\n",
    "\n",
    "For general implementation:\n",
    "- You should count `0` as positive\n",
    "- You may find it easier to convert your list of floats into 0 and 1 or True and False to denote positive/negative.  This is optional.\n",
    "- There are a number of ways to do this.  Given below is some example syntax via `itertools` (groupby).  You may also extract information from e.g. numpy's argsort, but don't import any packages not already in the header.  If your implementation is inefficient, you may find the later parts of this problem take a long time to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example usage for itertools.groupby on \"signs\" (NOT FLOATS)\n",
    "np.random.seed(2020)\n",
    "binarylist=np.random.choice(['+', '-'], size=14)\n",
    "print('Simulated signs:', binarylist)\n",
    "counts = np.array([(item, len(list(objects))) for item, objects in itertools.groupby(binarylist)])\n",
    "counts\n",
    "\n",
    "#your goal is to extract:\n",
    "#the number of distinct runs (looks like 7!)\n",
    "#the length of the longest run (looks like 5!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_runs(x):\n",
    "    #Do the thing!  Make sure 'x' can be floats, unlike the dummy example above\n",
    "    newX = [i>=0.0 for i in x]\n",
    "    cur = newX[0]\n",
    "    longestTemp = 1\n",
    "    longest = 1\n",
    "    count = 1\n",
    "    for i in newX[1:]:\n",
    "        if(cur==i):\n",
    "            longestTemp+=1\n",
    "            if (longestTemp>longest):\n",
    "                longest = longestTemp\n",
    "        else:\n",
    "            count +=1 \n",
    "            longestTemp = 1\n",
    "            cur = i\n",
    "    return count, longest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2 B: Trying out the statistic.**\n",
    "\n",
    "Try out your statistic on a few quick test cases to make sure it's working.\n",
    "\n",
    "Some examples to sanity check your function.\n",
    "\n",
    "- An input of [3,3,1,-1,-2] should return (2,3), since there are 2 runs (3 positive number then 2 negative numbers) the longest of which is the first 3.\n",
    "- An input of [1,-1,1,-1,1,-1] should return (6,1).\n",
    "- An input of [1,1,1,1,1,1] should return (1,6).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_runs([3,3,1,-1,-2]))\n",
    "print(count_runs([1,-1,1,-1,1,-1]))\n",
    "print(count_runs([1,1,1,1,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 C : Hypothesis Testing Setup**\n",
    "\n",
    "Our goal is to take the output of a regression model - usually in our `.resid` object - and decide whether or not the residuals are clumping or not.  State the null and alternative hypothesis for a test that presumes that the residuals are unpatterned, and rejects this only if there is significant statistical evidence to the contrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_0 : \\mu = \\mu_0 $ where $\\mu_0$ is some number of runs $\\newline$\n",
    "$H_1 : \\mu > \\mu_0$ or $\\mu < \\mu_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 D : Hypothesis Testing Simulations**\n",
    "\n",
    "We can use the `count_runs` function to determine a reasonable set of values for longest run and number of runs.  In other words: we need a rejection region for our test.  Simulate 1000 batches of 214 $N(0,1)$ random variables.  For each of the 1000 batch, compute the `count_runs` statistics of longest run and number of runs.\n",
    "\n",
    "Visualize the distributions of both statistics into side-by-side histograms.\n",
    "\n",
    "Then describe a **rejection region** for the the two-tailed $\\alpha=0.03$ hypothesis test from part 2C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "longests = []\n",
    "for i in range(1000):\n",
    "    data = np.random.normal(0,1,214)\n",
    "    temp = count_runs(data)\n",
    "    runs.append(temp[0])\n",
    "    longests.append(temp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,((ax0), (ax1)) = plt.subplots(1,2)\n",
    "\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(5)\n",
    "\n",
    "ax0.hist(runs,bins=10)\n",
    "ax1.hist(longests, bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbar = np.mean(runs)\n",
    "std = np.std(runs)\n",
    "adjstd = std/np.sqrt(1000)\n",
    "critVal = stats.norm.ppf(1.97/2)\n",
    "critVal2 = stats.norm.ppf(.03)\n",
    "high = xbar+critVal*adjstd\n",
    "low = xbar+critVal2*adjstd\n",
    "print(\"With an alpha of .03, and mu_0 = mean runs of the above test, the rejection region is mu < {l}, mu > {h})\".format(l=low,h=high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 E : Apply your test**\n",
    "\n",
    "Consider performing simple linear regression $y=\\beta_0+\\beta_1x$ for the `not_so_linear.csv` data from problem 1.  If we were to apply a runs rest to the residuals (`.resid`) from this model, what values do we get from `count_runs`?  Are they within the rejection region?  Should they be?\n",
    "\n",
    "**Note:**  You'll need to make sure you sort the data by increasing x before you apply this, because for regression we look for clumping of signs as the preditor increases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedDf = df.sort_values('x')\n",
    "y,x = dmatrices('y ~ x',data=df,return_type='dataframe')\n",
    "model = sm.OLS(y,x).fit()\n",
    "dfRuns = count_runs(model.resid)\n",
    "print(dfRuns)\n",
    "plt.scatter(sortedDf['x'],sortedDf['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values we recieve are 48 runs, with 9 being the longest count of runs, this is within the rejection region, thus rejecting the null hypothesis that our residuals are not clumping and confirming that the model fails one of the four assumptions of best fit linear regression. This makes sense as when sorted the data shows a potentially logrithmic relationship between x and y. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p3'></a>\n",
    "\n",
    "## [40 points] Problem 3: Multiple Linear Regression\n",
    "\n",
    "Load the data set in `houses`, with an opening snipper below.  The data includes sale prices of 24 houses from a midwetern town in the 1970's.   You are told that you need to quantify how price can be explained and predicted by the feature of the house.\n",
    "\n",
    "The variables available in the data set are labeled as follows:\n",
    "\n",
    "* sales:    the sales price of the house (in 1000s of dollars)\n",
    "* tax: \t    the local taxes\n",
    "* bath: \tthe number of bathrooms\n",
    "* lot: \t    the lot size (1000s of ft)\n",
    "* size: \tthe living space (1000s of ft)\n",
    "* garage: \tnumber of parking spots in the garage\n",
    "* rooms: \tnumber of rooms\n",
    "* bedrooms: number of bedrooms\n",
    "* age: \t    age in years\n",
    "* fire: \tnumber of fireplaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhouse=pd.read_csv('./data/houses.txt', delimiter = \"\\t\")\n",
    "dfhouse.columns=['tax','bath','lot','size','garage','rooms','bedrooms','age','fire','sales']\n",
    "dfhouse.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part 3 A: Explore**\n",
    "\n",
    "Make pairwise scatter plots of the continuous predictors/covariates, both against each other and against the outcome (expenditures).   Does the relationship between the independent variables and the dependent variables appear to be linear?  Do there appear to be independent variables that are collinear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dfhouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependent variables do seem to have a linear relationship with sales, the dependent variable. There do also appear to be independent collinear variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part 3 B: Make a Model**\n",
    "\n",
    "By adding columns to a minimal model or by subtracting columns from the full model, use one of the criteria in the class to create a reasonable candidate model.  These may include:\n",
    "\n",
    "- stepwise optimization of adjusted $R^2$\n",
    "- stepwise inclusion/removal of most or least-significant T-tests on coefficients\n",
    "- removing columns based on VIFs\n",
    "\n",
    "Use a markdown cell to explain exactly what method you're using to construct your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,x = dmatrices('sales ~ tax+bath+lot+size+garage+rooms+bedrooms+age+fire',data=dfhouse,return_type='dataframe')\n",
    "model = sm.OLS(y,x).fit()\n",
    "print(model.summary())\n",
    "for i in range(x.shape[1]):\n",
    "    print('VIF for ', x.columns[i], ' ', variance_inflation_factor(x.values, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,x = dmatrices('sales ~ lot+tax+bath+size+garage+bedrooms+age+fire',data=dfhouse,return_type='dataframe')\n",
    "model = sm.OLS(y,x).fit()\n",
    "model.summary()\n",
    "print(model.summary())\n",
    "for i in range(x.shape[1]):\n",
    "    print('VIF for ', x.columns[i], ' ', variance_inflation_factor(x.values, i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,x = dmatrices('sales ~ tax+bath+size+garage+bedrooms+age+fire',data=dfhouse,return_type='dataframe')\n",
    "model = sm.OLS(y,x).fit()\n",
    "print(model.summary())\n",
    "for i in range(x.shape[1]):\n",
    "    print('VIF for ', x.columns[i], ' ', variance_inflation_factor(x.values, i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,x = dmatrices('sales ~ tax+bath+size+garage+bedrooms+fire',data=dfhouse,return_type='dataframe')\n",
    "model = sm.OLS(y,x).fit()\n",
    "print(model.summary())\n",
    "for i in range(x.shape[1]):\n",
    "    print('VIF for ', x.columns[i], ' ', variance_inflation_factor(x.values, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,x = dmatrices('sales ~ tax+bath+garage+bedrooms+fire',data=dfhouse,return_type='dataframe')\n",
    "model = sm.OLS(y,x).fit()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I began with using VIFs, rooms had a VIF > 10 so I removed that first. After that all the VIFs were < 5, but I felt like the model was still bloated. I then started removing items based on p and t values, trying to make sure statistically insignificant variables were being dropped. I had originally dropped fireplaces as well, which increased the adj rsqd marginally but caused a larger drop in my r statistic. After going through most of part C with fire dropped I tried adding it again and felt like the model was better with it than without it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part 3 C: Validate your Model**\n",
    "\n",
    "\n",
    "Perform a thorough discussion of the underlying regression assumptions of your model in part 1B.  You should plot a predictor vs. residuals plot for each column and histogram OR qqplot of the overall residuals.  Make sure to also check for non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(model.resid,bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I feel like the residuals are being heavily affected by the fact that many of the predictors are values with large steps like bathrooms, garages, bedrooms, and fires. Causing gaps and funky peaks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tax+bath+garage+bedrooms+fire\n",
    "plt.scatter(dfhouse['tax'], model.resid)\n",
    "plt.hlines(0, xmin=min(dfhouse['tax']), xmax=max(dfhouse['tax']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems alright "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tax+bath+garage+bedrooms+fire\n",
    "plt.scatter(dfhouse['bath'], model.resid)\n",
    "plt.hlines(0, xmin=min(dfhouse['bath']), xmax=max(dfhouse['bath']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tax+bath+garage+bedrooms+fire\n",
    "plt.scatter(dfhouse['garage'], model.resid)\n",
    "plt.hlines(0, xmin=min(dfhouse['garage']), xmax=max(dfhouse['garage']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1.5 garages are kinda screwing it up, a $garages^2$ might help here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tax+bath+garage+bedrooms+fire\n",
    "plt.scatter(dfhouse['bedrooms'], model.resid)\n",
    "plt.hlines(0, xmin=min(dfhouse['bedrooms']), xmax=max(dfhouse['bedrooms']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could also potentially benefit from a polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dfhouse['fire'], model.resid)\n",
    "plt.hlines(0, xmin=min(dfhouse['fire']), xmax=max(dfhouse['fire']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part 3 D: Tune your Model**\n",
    "\n",
    "\n",
    "Based on your work in parts 1B and 1C, **iterate** on your model.  Consider removing terms or adding higher-order polynomials one at a time unless you are satisfied that your model captures the data as well as possible.  Each time you add or subtract a term from your model, you should repeat the steps in parts B and C: a summary table and exploration of assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhouse['bedroomssq'] = dfhouse['bedrooms']**2\n",
    "print(model.summary())\n",
    "y,x = dmatrices('sales ~ tax+bath+garage+fire+bedrooms+bedroomssq',data=dfhouse,return_type='dataframe')\n",
    "model2 = sm.OLS(y,x).fit()\n",
    "print(model2.summary())\n",
    "plt.scatter(dfhouse['bedrooms'], model2.resid)\n",
    "plt.hlines(0, xmin=min(dfhouse['bedrooms']), xmax=max(dfhouse['bedrooms']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a parabola on bedrooms seems to have really helped, r and adjusted rsqd are up, and the house vs residual plot looks far better but the f-statistic did go down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhouse['garagesq'] = dfhouse['garage']**2\n",
    "print(model.summary())\n",
    "y,x = dmatrices('sales ~ tax+bath+garage+fire+bedrooms+bedroomssq+garagesq',data=dfhouse,return_type='dataframe')\n",
    "model3 = sm.OLS(y,x).fit()\n",
    "print(model3.summary())\n",
    "plt.scatter(dfhouse['garage'], model3.resid)\n",
    "plt.hlines(0, xmin=min(dfhouse['garage']), xmax=max(dfhouse['garage']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't seem to help all that much "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part 3 E: Explain your Model**\n",
    "\n",
    "**Justify** your choices: there are a lot of ways to choose a \"best\" model: we've mentioned e.g. only including significant predictors versus F-tests versus optimizing R-squared.  Explain what terms you chose and why they were appropriate! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final model: $Sales=2.55(Tax)+7.43(Bath)+1.53(Garage)+1.72(Fire)+11.87(Bedrooms)-2.16(Bedroomssq) \\newline$\n",
    "All of the terms I chose helped increase my adj rsqd. And the terms, other than bedroomssq and maybe tax, are things you would see in a house listing. Bathrooms, garage space, bedrooms, and fires, are all things you would look at when valuing a house. The predictors I discarded age, size, lot, and rooms are well covered by the other predictors, or not all that important. I believe age to be less important as older homes may have been redone. Size and rooms are covered by having predictors for bath, bedrooms and tax. Tax also is a metric for lot size.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rubric notes:\n",
    "\n",
    "This problem is by design very open-ended.  It is meant to reflect a real-world problem solving process.  For this problem, at the very least, you should:\n",
    "- Include and explain which method you're using to removel redundant columns in part B\n",
    "- Include a sentence for **each** of the diagnositic plots in part C, determining whether your current model is meeting the standard assumptions of multiple linear regression\n",
    "- Include some code, sentences, and/or visualizations demonstrating what alternative models you tried and/or considered.\n",
    "- Include a sentence interpreting why your final model **makes sense** in the context of the problem - and it if doesn't, you'll want to try to explain why not.\n",
    "\n",
    "Graders will look for the both the diagnostic plots and **plenty** of complete English sentences describing what you're doing and why you think it will help answer the data science question: how do we create the best model we can to quantify how price can be explained and predicted by the features of the house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "[Back to top](#top)\n",
    "<a id='bot'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
